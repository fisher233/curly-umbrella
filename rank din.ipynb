{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from random import sample\n",
    "import datetime\n",
    "import time\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-22 15:40:42: Total Memrory:31.253597GB\n",
      "2020-09-22 15:40:42: Used Memory:17.729008GB\n",
      "2020-09-22 15:40:42: Free Memrory:7.232605GB\n"
     ]
    }
   ],
   "source": [
    "utils.show_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_mysql(sql):\n",
    "    host = '192.168.1.240' # '39.108.7.96'\n",
    "    port = 4000\n",
    "    user = 'biuser'  # zhangwenyu\n",
    "    password = 'biuser@2019'\n",
    "    database = 'charles'\n",
    "    utils.print_with_datetime('querying %s'%sql)\n",
    "    return utils.sql_to_df(sql, host, port, database, user ,password)\n",
    "\n",
    "def query_from_hive(sql):\n",
    "    host = '172.21.195.22'\n",
    "    port = 10000\n",
    "    db = 'source_logs'\n",
    "    user = 'zhangwenyu'\n",
    "    password = 'Zhangwy#123456'\n",
    "    utils.print_with_datetime('querying %s'%sql)\n",
    "    return utils.sql_to_df(sql, host=host,port=port, db=db, user=user,password=password, db_type='hive')\n",
    "\n",
    "def split_df_by_column(df, column, train_size=None, split_point=None):\n",
    "    points = sorted(df[column].values)\n",
    "    if split_point is None:\n",
    "        split_index = math.ceil(len(points) * train_size)\n",
    "        split_point = points[split_index]\n",
    "    df_train = df[df[column] < split_point]\n",
    "    df_test = df[df[column] >= split_point]\n",
    "    return df_train, df_test\n",
    "\n",
    "def sparseTensor(indices, values, dense_shape, dtype=np.int8):\n",
    "    l = np.zeros(dense_shape, dtype=dtype)\n",
    "    for i, sparse_indice in enumerate(indices):\n",
    "        if type(values) == int or type(values) == float:\n",
    "            l[sparse_indice[0]][sparse_indice[1]] = values\n",
    "        else:\n",
    "            l[sparse_indice[0]][sparse_indice[1]] = values[i]\n",
    "    return l\n",
    "\n",
    "class NHotEncoder(utils.LabelEncoder):\n",
    "    def __init__(self, table_=None):\n",
    "        super(NHotEncoder, self).__init__(table_)\n",
    "        \n",
    "    def fit(self, x):\n",
    "        for row in x:\n",
    "            super(NHotEncoder, self).fit(row)\n",
    "    \n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "    \n",
    "    def transform(self, labels, values=None):\n",
    "        batch_size = len(labels)\n",
    "        sparse_indices = []\n",
    "        sparse_values = []\n",
    "        \n",
    "        for i, row in enumerate(labels):\n",
    "            indices = super(NHotEncoder, self).transform(row)\n",
    "            row_indices = [[i, index] for index in indices]\n",
    "            sparse_indices += row_indices\n",
    "            if values is not None:\n",
    "                sparse_values += values[i]\n",
    "\n",
    "        output_shape = (batch_size, len(self.classes_))\n",
    "        if values is None:\n",
    "            sparse_values = 1\n",
    "        y = sparseTensor(indices=sparse_indices, values=sparse_values, dense_shape=output_shape)\n",
    "        return y\n",
    "\n",
    "\n",
    "def split_df_by_column(df, column, train_size=None, split_point=None):\n",
    "    points = sorted(df[column].values)\n",
    "    if split_point is None:\n",
    "        split_index = math.ceil(len(points) * train_size)\n",
    "        split_point = points[split_index]\n",
    "    df_train = df[df[column] < split_point]\n",
    "    df_test = df[df[column] >= split_point]\n",
    "    return df_train, df_test\n",
    "\n",
    "def normalize(df, fn, columns, suffix=None, inplace=False, param=None):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    for col in columns:\n",
    "        if suffix is not None:\n",
    "            col = col + '_' + suffix\n",
    "        if param is not None and col in param:\n",
    "#             print(param)\n",
    "            df[col] = fn(df[col], *param[col])\n",
    "        else:\n",
    "            df[col] = fn(df[col])\n",
    "\n",
    "    if not inplace:\n",
    "        return df\n",
    "\n",
    "def z_score(df, df_mean = None, df_std=None):\n",
    "    if df_mean is None or df_std is None:\n",
    "        return (df - df.mean()) / df.std()\n",
    "    else:\n",
    "        return (df - df_mean) / df_std\n",
    "    \n",
    "def min_max(df, df_min=None, df_max=None):\n",
    "    if df_min is None or df_max is None:\n",
    "        return (df - df.min()) / (df.max() - df.min())\n",
    "    else:\n",
    "        return (df - df_min) / (df_max - df_min)\n",
    "    \n",
    "def time2sec(t):\n",
    "    hms = t.strip().split(\":\")\n",
    "    if len(hms) == 3:\n",
    "        return int(hms[0]) * 3600 + int(hms[1]) * 60 + int(hms[2])\n",
    "    elif len(hms) == 2:\n",
    "        return int(hms[0]) * 60 + int(hms[1])\n",
    "    else:\n",
    "        return int(hms[0])\n",
    "    \n",
    "def get_hot_cols(col, n):\n",
    "    return [col+'_'+str(i) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make psudo data\n",
    "# utvid_indexes = list(range(len(encoder.classes_))) # df_train[y_col].unique()\n",
    "def watch_window_filter(x, window_size=30, targe_col = 'utvId_past', time_col='event_time_past'):\n",
    "    # sort by time and get the target data within time window size\n",
    "    sorted_indices = np.argsort(x[time_col])[-window_size:]\n",
    "    return np.array(x[targe_col])[sorted_indices]\n",
    "\n",
    "def get_neg_data(df, uid_utvid, utvid_indexes, neg_times=1):\n",
    "    n_neg = df.shape[0] * neg_times\n",
    "#     arr_neg = np.zeros((n_neg, df.shape[1]), dtype=int)\n",
    "    arr_neg = np.empty_like(df, shape=(n_neg, df.shape[1]))\n",
    "    neg_item_ids = []\n",
    "    for i in range(len(df)):\n",
    "        if i % 50000 == 0:\n",
    "            utils.print_with_datetime(f'Finished {i} negative data')\n",
    "        non_positive_items = np.setdiff1d(utvid_indexes, uid_utvid[df.iloc[i]['uid']], assume_unique=True)\n",
    "        non_positive_item_ids = np.random.choice(non_positive_items, neg_times, replace=False)\n",
    "        neg_item_ids.extend(non_positive_item_ids)\n",
    "        off_set = i * neg_times\n",
    "        for j in range(neg_times):\n",
    "            o = off_set + j\n",
    "            for c in range(len(df.columns)):\n",
    "                arr_neg[o,c] = df.iloc[i,c]\n",
    "#             df_neg.iloc[o]['utvId_now'] = non_positive_item_ids[j]\n",
    "    df_neg = pd.DataFrame(arr_neg, columns=df.columns)\n",
    "    df_neg['utvId_now'] = neg_item_ids\n",
    "    return df_neg\n",
    "\n",
    "def get_data(neg_times=1, embedding_space={}, hot_space={}):\n",
    "    sql = 'select uid, utvId, event_time, areaId, completed, collect_day,duration_seconds from xsyx_report_skuinfo.video_user_clean2'\n",
    "    df_clean =  query_from_mysql(sql)\n",
    "#     num_video = \n",
    "    df_clean2 = df_clean.copy()\n",
    "    utils.print_with_datetime('Merging df')\n",
    "    df_merged = df_clean.merge(df_clean2,on=['uid','areaId'], suffixes=('_now','_past'))\n",
    "    \n",
    "    # purchasing history timestamp should smaller than this purchasing timestamp\n",
    "    df_merged = df_merged[df_merged['event_time_past']<df_merged['event_time_now']]\n",
    "    utils.print_with_datetime('Grouping df')\n",
    "    df_pos = df_merged.groupby(['uid','areaId','event_time_now'])\\\n",
    "                            .agg({ 'utvId_now': lambda col: col.tolist()[0],\n",
    "                                 'completed_now': lambda col: col.tolist()[0],\n",
    "                                  'collect_day_now': lambda col: col.tolist()[0],\n",
    "                                  'utvId_past':lambda col: col.tolist(), \n",
    "                                  'event_time_past':lambda col: col.tolist(), \n",
    "#                                  'completed_past': lambda col: col.tolist(),\n",
    "#                                 'collect_day_past': lambda col: col.tolist(),\n",
    "                                 'duration_seconds_past': lambda col: col.tolist()})\\\n",
    "                            .reset_index()\n",
    "    \n",
    "    df_pos['is_weekend'] = df_pos['event_time_now'].apply(lambda x: 1 if datetime.datetime.fromtimestamp(x/1e3).isoweekday()>5 else 0)\n",
    "#     df_pos.to_csv('df_pos.csv', index=False)\n",
    "#     df_pos = pd.read_csv('df_pos.csv')\n",
    "    utils.print_with_datetime('Filtering watch windows')\n",
    "#     df_pos['utvId_past'] = df_pos.apply(watch_window_filter, axis=1)\n",
    "    embedding_space = {'tagId':{'embedding_size':4}, 'utvid':{'embedding_size':128}, 'authorId':{'embedding_size':4},\n",
    "                       'areaId':{'embedding_size':3}} #, 'uid':{'embedding_size':8}\n",
    "    encoder = utils.LabelEncoder()\n",
    "    encoder.fit(df_pos['utvId_now'].values)\n",
    "    embedding_space['utvid']['encoder'] = encoder\n",
    "#     hot_space['utvid'] = {'encoder': encoder}\n",
    "#     item_embedding_shape = []\n",
    "    user_time = {}\n",
    "    user_time_item = {}\n",
    "    item = {}\n",
    "    \n",
    "    user_time['pk'] = {'uid','collect_day_now','event_time_now'}\n",
    "    user_time['categorical'] = {'multivalent':{'embedding': [{'space_id':'utvid', 'col':'utvId_past'}]},\n",
    "                                'univalent':{'embedding':[{'space_id':'areaId', 'col':'areaId'}], \n",
    "                                             'raw':['is_weekend']}}\n",
    "#     user_time['categorical'] = {'multivalent':{'hot': [{'space_id':'utvid', 'col':'utvId_past'}]},\n",
    "#                                 'univalent':{'hot':[{'space_id':'areaId', 'col':'areaId'}],\n",
    "#                                                     {'space_id':'uid', 'col':'uid'}], \n",
    "#                                              'raw':['is_weekend']}}\n",
    "    user_time['continuous'] = {'aggregative':['duration_seconds_past']}\n",
    "#     user_time['data'] = df[['uid','collect_day_now','event_time_now','utvId_past','areaId','duration_seconds_past']]\n",
    "    \n",
    "    user_time_item['categorical'] = {'univalent':{'embedding': [{'space_id':'utvid', 'col':'utvId_now'}]}}\n",
    "    user_time_item['reference'] = {'user_time':{'uid':'uid', 'event_time_now':'event_time_now'},\n",
    "                                   'item':{'utvId_now':'utvId'}} # this_col:that_col\n",
    "#     user_time_item['data'] = df[['uid','event_time_now','utvId_now']]\n",
    "    \n",
    "    sql_utvid_auth = 'SELECT id, authorId, likeNum, playNum, duration, tmCreate as uploadtime FROM xsyx_frxs_base.t_utv'\n",
    "    df_utvid_auth = query_from_mysql(sql_utvid_auth)\n",
    "#     df = df.merge(df_utvid_auth, left_on='utvId_now', right_on='id', how='left')\n",
    "    sql_utvid_tags = 'SELECT utvId, tagId FROM xsyx_frxs_base.t_utv_tag_rel v'\n",
    "    df_utvid_tags = query_from_mysql(sql_utvid_tags)\n",
    "    \n",
    "    df_utv = df_utvid_auth.merge(df_utvid_tags, left_on='id', right_on='utvId', how='inner')\n",
    "    del df_utv['id']\n",
    "    df_utv = df_utv.groupby(['utvId','authorId', 'likeNum', 'playNum','duration','uploadtime']).agg({'tagId': lambda col: col.tolist()}).reset_index()\n",
    "    df_utv['duration_seconds'] = df_utv['duration'].apply(lambda x : time2sec(x))\n",
    "    \n",
    "    item['pk'] = {'utvId'}\n",
    "    item['categorical'] = {'multivalent':{'embedding': [{'space_id':'tagId', 'col':'tagId'}]},\n",
    "                           'univalent':{'embedding':[{'space_id':'authorId', 'col':'authorId'}],\n",
    "                                        'raw': ['uploadtime']}}\n",
    "#     item['categorical'] = {'multivalent':{'hot': [{'space_id':'tagId', 'col':'tagId'}]},\n",
    "#                            'univalent':{'hot':[{'space_id':'authorId', 'col':'authorId'}]}}\n",
    "    item['continuous'] = {'raw':['likeNum', 'playNum', 'duration_seconds']}\n",
    "#     item['data'] = df_utv\n",
    "    \n",
    "    utvid_indexes = df_pos['utvId_now'].unique()\n",
    "    uid_utvid = {}\n",
    "    for uid in df_pos['uid'].unique():\n",
    "        df_uid = df_pos[df_pos['uid'] == uid]\n",
    "        video_ids_watched = np.concatenate(df_uid['utvId_past'].values)\n",
    "        video_ids_watched = np.union1d(video_ids_watched, df_uid['utvId_now'].values) # result is already unique sorted \n",
    "        uid_utvid[uid] = list(video_ids_watched) # set not work for np.setdiff1d(a,b) where a and b must be list\n",
    "\n",
    "    df_pos['y'] = 1\n",
    "    if neg_times > 0:\n",
    "        df_neg = get_neg_data(df_pos, uid_utvid, utvid_indexes, neg_times=neg_times)\n",
    "        df_neg['y'] = 0\n",
    "        print(df_pos.shape, df_neg.shape)\n",
    "        df = pd.concat([df_pos, df_neg])\n",
    "        df = df.reset_index() # ValueError: Shape of passed values is (1797396, 20), indices imply (1198264, 20)\n",
    "    else:\n",
    "        df = df_pos\n",
    "        \n",
    "    nested_data = [{'data':df_pos, 'features': {'user_time':user_time}},\n",
    "                   {'data':df, 'features': {'user_time_item':user_time_item}},\n",
    "                   {'data':df_utv, 'features':{'item':item}}]\n",
    "#     embedding_size = {'tagId':4, 'utvid':128}\n",
    "    \n",
    "    utils.print_with_datetime(f'Finished getting data')\n",
    "    return nested_data, embedding_space, hot_space\n",
    "    \n",
    "def process_data(nested_data, split_fn=None, topk_preds=None, need_scale=True,\n",
    "                 cont_cols = [], scaler=None, embedding_space={}, hot_space={}, \n",
    "                 pk={}, reference={}):\n",
    "    # neg_kv = neg_key: {neg_values: neg_samples}, e.g. uid: {utvId_now:[neg_samples]}\n",
    "    # hot_space = col: encoder\n",
    "    # embedding_space = [space_id: {'embedding_size':embedding_size, 'encoder':encoder,\n",
    "    #                               'univalent':col, 'multivalent':col}]\n",
    "\n",
    "    # pk = {name: dataframe}\n",
    "    # reference = {'reference':{this_col:that_col}, 'data':dataframe}\n",
    "    \n",
    "    for data in nested_data:\n",
    "        features = data['features']\n",
    "        df = data['data']\n",
    "        \n",
    "        for name, feats in features.items():\n",
    "            print(name, df.shape)\n",
    "            df_list = []\n",
    "            \n",
    "            utils.print_with_datetime(f'Processing {name}')\n",
    "            if 'categorical' in feats:\n",
    "                categorical_feats = feats['categorical']\n",
    "                if 'univalent' in categorical_feats:\n",
    "                    uni_cate_feats = categorical_feats['univalent']\n",
    "                    if 'hot' in uni_cate_feats:\n",
    "                        uni_cate_feats_hot = uni_cate_feats['hot']\n",
    "                        for space in uni_cate_feats_hot:\n",
    "                            space_id = space['space_id']\n",
    "                            col = space['col']\n",
    "                            if space_id not in hot_space:\n",
    "                                hot_space[space_id] = {}\n",
    "                            if 'encoder' in hot_space[space_id]:\n",
    "                                one_hot_encoder = hot_space[space_id]['encoder']\n",
    "                                if type(one_hot_encoder) == utils.LabelEncoder:\n",
    "                                    one_hot_encoder = utils.OneHotEncoder(one_hot_encoder.table_)\n",
    "                            else:\n",
    "                                one_hot_encoder = utils.OneHotEncoder()\n",
    "                                one_hot_encoder.fit(df[col].values)\n",
    "                                hot_space[space_id]['encoder'] = one_hot_encoder\n",
    "                            one_hot_values = one_hot_encoder.transform(df[col].values)\n",
    "#                             df = pd.concat([df, pd.DataFrame(one_hot_values)], axis=1)\n",
    "                            df_tmp = pd.DataFrame(one_hot_values, columns=get_hot_cols(col, len(one_hot_encoder.classes_)))\n",
    "#                             print(130, df_tmp.shape)\n",
    "                            df_list.append(df_tmp)\n",
    "\n",
    "                    if 'embedding' in uni_cate_feats:\n",
    "                        uni_cate_feats_emb = uni_cate_feats['embedding']\n",
    "                        for embedding in uni_cate_feats_emb:\n",
    "                            space_id = embedding['space_id']\n",
    "                            col = embedding['col']\n",
    "                            if space_id not in embedding_space:\n",
    "                                embedding_space[space_id] = {}\n",
    "                            \n",
    "                            if 'encoder' in embedding_space[space_id]:\n",
    "                                encoder = embedding_space[space_id]['encoder']\n",
    "                            else:\n",
    "                                encoder = utils.LabelEncoder()\n",
    "                                encoder.fit(df[col].values)\n",
    "                                embedding_space[space_id]['encoder'] = encoder\n",
    "                            \n",
    "                            df_tmp = pd.DataFrame(encoder.transform(df[col].values), columns=[col+'_emb'])\n",
    "                            df_list.append(df_tmp)\n",
    "\n",
    "                        embedding_space[space_id]['univalent'] = col+'_emb'\n",
    "                        \n",
    "                    if 'raw' in uni_cate_feats:\n",
    "                        uni_cate_feats_raw = uni_cate_feats['raw']\n",
    "                        df_list.append(df[uni_cate_feats_raw])\n",
    "                \n",
    "                if 'multivalent' in categorical_feats:\n",
    "                    multi_cate_feats = categorical_feats['multivalent']\n",
    "                    if 'hot' in multi_cate_feats:\n",
    "                        multi_cate_feats_hot = multi_cate_feats['hot']\n",
    "                        for space in multi_cate_feats_hot:\n",
    "                            space_id = space['space_id']\n",
    "                            col = space['col']\n",
    "                            if space_id not in hot_space:\n",
    "                                hot_space[space_id] = {}\n",
    "                            if 'encoder' in hot_space[space_id]:\n",
    "                                n_hot_encoder = hot_space[space_id]['encoder']\n",
    "                                if type(n_hot_encoder) == utils.LabelEncoder:\n",
    "                                    n_hot_encoder = NHotEncoder(n_hot_encoder.table_)\n",
    "                            else:\n",
    "                                n_hot_encoder = NHotEncoder()\n",
    "                                n_hot_encoder.fit(df[col].values)\n",
    "                                hot_space[space_id]['encoder'] = n_hot_encoder\n",
    "                            print(col)\n",
    "                            n_hot_values = n_hot_encoder.transform(df[col].values)\n",
    "#                             df = pd.concat([df, pd.DataFrame(n_hot_values)], axis=1)\n",
    "                            print(164, n_hot_values.shape)\n",
    "                            df_list.append(pd.DataFrame(n_hot_values, columns=get_hot_cols(col, len(n_hot_encoder.classes_))))\n",
    "                            \n",
    "                    if 'embedding' in multi_cate_feats:\n",
    "                        multi_cate_feats_emb = multi_cate_feats['embedding']\n",
    "                        for embedding in multi_cate_feats_emb:\n",
    "                            space_id = embedding['space_id']\n",
    "                            col = embedding['col']\n",
    "                            if space_id not in embedding_space:\n",
    "                                embedding_space[space_id] = {}\n",
    "                            if 'encoder' in embedding_space[space_id]:\n",
    "                                encoder = embedding_space[space_id]['encoder']\n",
    "                            else:\n",
    "                                encoder = utils.LabelEncoder()\n",
    "                                for i in range(len(df[col])):\n",
    "                                    encoder.fit(df[col].iloc[i])\n",
    "                                embedding_space[space_id]['encoder'] = encoder\n",
    "\n",
    "                            df_tmp = pd.DataFrame(df[col].apply(lambda x : encoder.transform(x)).values, columns=[col+'_emb'])\n",
    "                            df_list.append(df_tmp)\n",
    "                            embedding_space[space_id]['multivalent'] = col+'_emb'\n",
    "                            \n",
    "            if 'continuous' in feats:\n",
    "                continuous_feats = feats['continuous']\n",
    "                if 'aggregative' in continuous_feats:\n",
    "                    agg_cont_feats = continuous_feats['aggregative']\n",
    "                    for col in agg_cont_feats:\n",
    "#                         df[col+'_mean'] = df[col].apply(lambda l : np.mean(l))\n",
    "#                         df[col+'_min'] = df[col].apply(lambda l : min(l))\n",
    "#                         df[col+'_max'] = df[col].apply(lambda l : max(l))\n",
    "                        df_list.append(pd.DataFrame(df[col].apply(lambda l : np.mean(l)).values, columns=[col+'_mean']))\n",
    "                        df_list.append(pd.DataFrame(df[col].apply(lambda l : min(l)).values, columns=[col+'_min']))\n",
    "                        df_list.append(pd.DataFrame(df[col].apply(lambda l : max(l)).values, columns=[col+'_max']))\n",
    "                        cont_cols.extend([col+'_mean', col+'_min', col+'_max'])\n",
    "                if 'raw' in continuous_feats:\n",
    "                    raw_cont_feats = continuous_feats['raw']\n",
    "                    df_list.append(df[raw_cont_feats])\n",
    "                    cont_cols.extend(raw_cont_feats)\n",
    "                # norm\n",
    "            if 'reference' in feats:\n",
    "                cols = [x for v in feats['reference'].values() for x in v.keys()]\n",
    "#                 print(feats['reference'].values())\n",
    "#                 print(cols)\n",
    "                df_list.append(df[cols])\n",
    "                reference['data'] = pd.concat(df_list, axis=1)\n",
    "                if 'y' in df:\n",
    "                    reference['data']['y'] = df['y']\n",
    "                reference['reference'] = feats['reference']\n",
    "            elif 'pk' in feats:\n",
    "                cols = list(feats['pk'])\n",
    "#                 print(feats['pk'])\n",
    "#                 print(cols)\n",
    "                df_list.append(df[cols])\n",
    "#                 return df_list\n",
    "#                 for l in df_list:\n",
    "#                     print(l.shape)\n",
    "                pk[name] = pd.concat(df_list, axis=1)\n",
    "        \n",
    "    processed_data = reference['data']\n",
    "    print(processed_data.shape)\n",
    "    for name, cols in reference['reference'].items():\n",
    "        print(cols, processed_data.columns)\n",
    "        print(name, pk[name].shape, pk[name].columns)\n",
    "        processed_data = processed_data.merge(pk[name], left_on = list(cols.keys()), right_on=list(cols.values()), how='left')\n",
    "        print(processed_data.shape)\n",
    "    if split_fn is not None:\n",
    "        df_train, df_test = split_fn(processed_data)\n",
    "        \n",
    "        if len(cont_cols) > 0 and need_scale:\n",
    "            if scaler is None:\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(df_train[cont_cols])\n",
    "            df_train[cont_cols] = scaler.transform(df_train[cont_cols])\n",
    "            df_test[cont_cols] = scaler.transform(df_test[cont_cols])\n",
    "            utils.print_with_datetime(f'Finished processing data')\n",
    "            return df_train, df_test, reference, pk, cont_cols, scaler, embedding_space, hot_space\n",
    "        return df_train, df_test, reference, pk, cont_cols, embedding_space, hot_space\n",
    "    \n",
    "    if len(cont_cols) > 0 and need_scale:\n",
    "        if scaler is None:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(processed_data[cont_cols])\n",
    "        processed_data[cont_cols] = scaler.transform(processed_data[cont_cols])\n",
    "\n",
    "    return processed_data, reference, pk, cont_cols, embedding_space, hot_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-22 09:59:26: querying select uid, utvId, event_time, areaId, completed, collect_day,duration_seconds from xsyx_report_skuinfo.video_user_clean2\n",
      "2020-09-22 09:59:30: Merging df\n",
      "2020-09-22 09:59:33: Grouping df\n",
      "2020-09-22 10:02:43: Filtering watch windows\n",
      "2020-09-22 10:02:43: querying SELECT id, authorId, likeNum, playNum, duration, tmCreate as uploadtime FROM xsyx_frxs_base.t_utv\n",
      "2020-09-22 10:02:43: querying SELECT utvId, tagId FROM xsyx_frxs_base.t_utv_tag_rel v\n",
      "2020-09-22 10:04:51: Finished 0 negative data\n",
      "2020-09-22 10:05:24: Finished 50000 negative data\n",
      "2020-09-22 10:05:59: Finished 100000 negative data\n",
      "2020-09-22 10:06:33: Finished 150000 negative data\n",
      "2020-09-22 10:07:06: Finished 200000 negative data\n",
      "2020-09-22 10:07:40: Finished 250000 negative data\n",
      "2020-09-22 10:08:14: Finished 300000 negative data\n",
      "2020-09-22 10:08:48: Finished 350000 negative data\n",
      "2020-09-22 10:09:22: Finished 400000 negative data\n",
      "2020-09-22 10:09:56: Finished 450000 negative data\n",
      "2020-09-22 10:10:30: Finished 500000 negative data\n",
      "2020-09-22 10:11:04: Finished 550000 negative data\n",
      "(599132, 11) (2396528, 11)\n",
      "2020-09-22 10:11:43: Finished getting data\n",
      "user_time (599132, 11)\n",
      "2020-09-22 10:11:43: Processing user_time\n",
      "user_time_item (2995660, 12)\n",
      "2020-09-22 10:11:52: Processing user_time_item\n",
      "item (939, 8)\n",
      "2020-09-22 10:11:55: Processing item\n",
      "(2995660, 5)\n",
      "{'uid': 'uid', 'event_time_now': 'event_time_now'} Index(['utvId_now_emb', 'uid', 'event_time_now', 'utvId_now', 'y'], dtype='object')\n",
      "user_time (599132, 9) Index(['areaId_emb', 'is_weekend', 'utvId_past_emb',\n",
      "       'duration_seconds_past_mean', 'duration_seconds_past_min',\n",
      "       'duration_seconds_past_max', 'event_time_now', 'uid',\n",
      "       'collect_day_now'],\n",
      "      dtype='object')\n",
      "(2995660, 12)\n",
      "{'utvId_now': 'utvId'} Index(['utvId_now_emb', 'uid', 'event_time_now', 'utvId_now', 'y',\n",
      "       'areaId_emb', 'is_weekend', 'utvId_past_emb',\n",
      "       'duration_seconds_past_mean', 'duration_seconds_past_min',\n",
      "       'duration_seconds_past_max', 'collect_day_now'],\n",
      "      dtype='object')\n",
      "item (939, 7) Index(['authorId_emb', 'uploadtime', 'tagId_emb', 'likeNum', 'playNum',\n",
      "       'duration_seconds', 'utvId'],\n",
      "      dtype='object')\n",
      "(2995660, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:290: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:291: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-22 10:12:07: Finished processing data\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "data, embedding_space, hot_space = get_data(neg_times=4)\n",
    "df_user_time_item = data[0]['data']\n",
    "split_fn = lambda df: split_df_by_column(df, column='collect_day_now', split_point='2020-07-29')\n",
    "\n",
    "df_train, df_test, reference, pk, cont_cols, scaler, embedding_space, hot_space = \\\n",
    "                                        process_data(data, split_fn=split_fn, embedding_space=embedding_space, hot_space=hot_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(df_train, col_fn_map, axis=1, need_scale=True, df_test=None, scaler=None, col_fn_map_test=None):\n",
    "    for col, fn in col_fn_map.items():\n",
    "        df_train[col] = df_train.apply(fn, axis=axis)\n",
    "        if df_test is not None:\n",
    "            df_test[col] = df_test.apply(fn, axis=axis)\n",
    "\n",
    "\n",
    "    cols = list(col_fn_map.keys())\n",
    "    if need_scale:\n",
    "        if scaler is None:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(df_train[cols])\n",
    "        df_train[cols] = scaler.transform(df_train[cols])\n",
    "        if df_test is not None:\n",
    "            df_test[cols] = scaler.transform(df_test[cols])\n",
    "    \n",
    "    if df_test is not None:\n",
    "        return df_train, df_test, scaler\n",
    "    else:\n",
    "        return df_train, scaler\n",
    "\n",
    "event_time_now_max = df_train['event_time_now'].max()\n",
    "col_fn_map = {'now_uploadtime': lambda x: x['event_time_now']//1e3 - datetime.datetime.strptime(str(x['uploadtime']), '%Y-%m-%d %H:%M:%S').timestamp(),\n",
    "              'example_age': lambda x: (event_time_now_max - x['event_time_now']) / 1000, \n",
    "              'example_age_square': lambda x: x['example_age']**2,\n",
    "              'example_age_rootsquare': lambda x: x['example_age']**0.5}\n",
    "\n",
    "df_train, post_scaler = postprocess(df_train=df_train, col_fn_map=col_fn_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utvId_now_emb</th>\n",
       "      <th>uid</th>\n",
       "      <th>event_time_now</th>\n",
       "      <th>utvId_now</th>\n",
       "      <th>y</th>\n",
       "      <th>areaId_emb</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>utvId_past_emb</th>\n",
       "      <th>duration_seconds_past_mean</th>\n",
       "      <th>duration_seconds_past_min</th>\n",
       "      <th>...</th>\n",
       "      <th>uploadtime</th>\n",
       "      <th>tagId_emb</th>\n",
       "      <th>likeNum</th>\n",
       "      <th>playNum</th>\n",
       "      <th>duration_seconds</th>\n",
       "      <th>utvId</th>\n",
       "      <th>example_age</th>\n",
       "      <th>example_age_square</th>\n",
       "      <th>example_age_rootsquare</th>\n",
       "      <th>now_uploadtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11128</td>\n",
       "      <td>1594457113017</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[133]</td>\n",
       "      <td>0.361868</td>\n",
       "      <td>1.329460</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-07-09 18:01:29</td>\n",
       "      <td>[71, 2]</td>\n",
       "      <td>-0.304718</td>\n",
       "      <td>0.612053</td>\n",
       "      <td>0.055494</td>\n",
       "      <td>307</td>\n",
       "      <td>1.456628</td>\n",
       "      <td>1.637005</td>\n",
       "      <td>1.288222</td>\n",
       "      <td>-0.737570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11146</td>\n",
       "      <td>1594298361199</td>\n",
       "      <td>312</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[104]</td>\n",
       "      <td>0.240351</td>\n",
       "      <td>1.221383</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-07-09 20:26:35</td>\n",
       "      <td>[81]</td>\n",
       "      <td>-0.583292</td>\n",
       "      <td>-0.584395</td>\n",
       "      <td>-1.502604</td>\n",
       "      <td>312</td>\n",
       "      <td>1.753494</td>\n",
       "      <td>2.206744</td>\n",
       "      <td>1.468151</td>\n",
       "      <td>-0.823698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11146</td>\n",
       "      <td>1594298428665</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[104, 1]</td>\n",
       "      <td>-1.015318</td>\n",
       "      <td>-1.012219</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-07-09 17:56:56</td>\n",
       "      <td>[71, 48]</td>\n",
       "      <td>-0.263229</td>\n",
       "      <td>-0.540057</td>\n",
       "      <td>-0.299861</td>\n",
       "      <td>305</td>\n",
       "      <td>1.753368</td>\n",
       "      <td>2.206490</td>\n",
       "      <td>1.468076</td>\n",
       "      <td>-0.819045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11146</td>\n",
       "      <td>1594305729006</td>\n",
       "      <td>306</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[104, 1, 2]</td>\n",
       "      <td>-0.839795</td>\n",
       "      <td>-1.012219</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-07-09 17:58:37</td>\n",
       "      <td>[48]</td>\n",
       "      <td>-0.384734</td>\n",
       "      <td>-0.363005</td>\n",
       "      <td>-0.846562</td>\n",
       "      <td>306</td>\n",
       "      <td>1.739716</td>\n",
       "      <td>2.179031</td>\n",
       "      <td>1.459996</td>\n",
       "      <td>-0.815342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11146</td>\n",
       "      <td>1594611421133</td>\n",
       "      <td>338</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[104, 1, 2, 3]</td>\n",
       "      <td>-0.954560</td>\n",
       "      <td>-1.012219</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-07-12 16:59:19</td>\n",
       "      <td>[48]</td>\n",
       "      <td>-0.435115</td>\n",
       "      <td>-0.606376</td>\n",
       "      <td>0.629530</td>\n",
       "      <td>338</td>\n",
       "      <td>1.168071</td>\n",
       "      <td>1.138276</td>\n",
       "      <td>1.103917</td>\n",
       "      <td>-0.789600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   utvId_now_emb    uid event_time_now  utvId_now  y  areaId_emb  is_weekend  \\\n",
       "0              0  11128  1594457113017        307  1           0           1   \n",
       "1              1  11146  1594298361199        312  1           0           0   \n",
       "2              2  11146  1594298428665        305  1           0           0   \n",
       "3              3  11146  1594305729006        306  1           0           0   \n",
       "4              4  11146  1594611421133        338  1           0           0   \n",
       "\n",
       "   utvId_past_emb  duration_seconds_past_mean  duration_seconds_past_min  ...  \\\n",
       "0           [133]                    0.361868                   1.329460  ...   \n",
       "1           [104]                    0.240351                   1.221383  ...   \n",
       "2        [104, 1]                   -1.015318                  -1.012219  ...   \n",
       "3     [104, 1, 2]                   -0.839795                  -1.012219  ...   \n",
       "4  [104, 1, 2, 3]                   -0.954560                  -1.012219  ...   \n",
       "\n",
       "           uploadtime tagId_emb   likeNum   playNum duration_seconds  utvId  \\\n",
       "0 2020-07-09 18:01:29   [71, 2] -0.304718  0.612053         0.055494    307   \n",
       "1 2020-07-09 20:26:35      [81] -0.583292 -0.584395        -1.502604    312   \n",
       "2 2020-07-09 17:56:56  [71, 48] -0.263229 -0.540057        -0.299861    305   \n",
       "3 2020-07-09 17:58:37      [48] -0.384734 -0.363005        -0.846562    306   \n",
       "4 2020-07-12 16:59:19      [48] -0.435115 -0.606376         0.629530    338   \n",
       "\n",
       "   example_age  example_age_square  example_age_rootsquare  now_uploadtime  \n",
       "0     1.456628            1.637005                1.288222       -0.737570  \n",
       "1     1.753494            2.206744                1.468151       -0.823698  \n",
       "2     1.753368            2.206490                1.468076       -0.819045  \n",
       "3     1.739716            2.179031                1.459996       -0.815342  \n",
       "4     1.168071            1.138276                1.103917       -0.789600  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1107798, 21), (90466, 21))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(X):\n",
    "    out = tf.keras.layers.Dense(X.shape[1], activation=tf.nn.relu)(X)\n",
    "    out = tf.keras.layers.Dense(X.shape[1])(X)\n",
    "    return tf.nn.relu(X + out)\n",
    "\n",
    "def deep_crossing(X, n_res_block=5):\n",
    "    for i in range(n_res_block):\n",
    "        X = residual_block(X)\n",
    "    return X\n",
    "\n",
    "def wide_and_deep(X, wide_network, deep_network, combination_layer):\n",
    "    return combination_layer(wide_network(X), deep_network(X))\n",
    "    \n",
    "def wide_crossing(X, n_cross=5):\n",
    "    X_l = X\n",
    "    for i in range(n_cross):\n",
    "        W_l = tf.Variable(tf.random.truncated_normal([X.shape[1]], stddev=0.01), name = f'cross_weight_{i}')\n",
    "        b_l = tf.Variable(tf.zeros_initializer()(shape=[X.shape[1]]), name = f'cross_bias_{i}')\n",
    "        X_l = X * X_l * W_l + b_l + X_l\n",
    "\n",
    "    return X_l\n",
    "\n",
    "def combination_layer(wide_out, deep_out):\n",
    "    return tf.concat([wide_out, deep_out], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.concatenate([np.ones([4,3]),np.zeros([4,2])], axis=-1)\n",
    "key = np.ones([4,5])\n",
    "q = np.ones([5])\n",
    "key * q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.where([True,False,True],[1,2,4],[3,2,5]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(embedding_shapes, num_layers = [2048,1024,512,256], k = 10, init_checkpoint=None, learning_rate=1e-4, \n",
    "                     drop_out_rate = 0.2, num_train_steps=None, num_warmup_steps=None, max_seq_length = 50):\n",
    "    \n",
    "    def model_fn(features, labels, mode):\n",
    "#         embedding_shapes = {space_id:embedding_shape}\n",
    "#         features = {'cont_features':tensor, \n",
    "#                     'embedding_features':{space_id: {'fixed':tensor, 'ragged':sparse_tensor}}}\n",
    "        cont_features = features['cont_features']\n",
    "        embedding_features = features['embedding_features']\n",
    "#         print(features)\n",
    "        top_k = k\n",
    "        embeddings = []\n",
    "        attention = {}\n",
    "        for embeddings_name, embeddings_value in embedding_features.items():\n",
    "            embedding_shape = embedding_shapes[embeddings_name]['embedding_shape']\n",
    "            embedding = tf.Variable(\n",
    "                            tf.random.truncated_normal(embedding_shape, stddev=0.01), # 1.0 / math.sqrt(embedding_shape[1])\n",
    "                            name = embeddings_name)\n",
    "            if 'univalent' in embeddings_value:\n",
    "                embedding_fixed_ids = embeddings_value['univalent']\n",
    "                embedding_fixed = tf.nn.embedding_lookup(embedding, embedding_fixed_ids)\n",
    "                bias = tf.Variable(tf.zeros_initializer()(shape=[1]), name=f'{embeddings_name}_univalent_bias')\n",
    "                embedding_fixed = tf.nn.relu(embedding_fixed + bias)\n",
    "                embeddings.append(embedding_fixed)\n",
    "                \n",
    "            if 'multivalent' in embeddings_value:\n",
    "                embedding_ragged_ids = embeddings_value['multivalent']\n",
    "                embedding_ragged = tf.nn.embedding_lookup_sparse(embedding, embedding_ragged_ids, None, combiner=\"mean\")\n",
    "                bias = tf.Variable(tf.zeros_initializer()(shape=[1]), name=f'{embeddings_name}_multivalent_bias')\n",
    "                embedding_ragged = tf.nn.relu(embedding_ragged + bias)\n",
    "                embeddings.append(embedding_ragged)\n",
    "            \n",
    "            if 'fixed_id' in embeddings_value:\n",
    "                embedding_fixed_ids_truncated = embeddings_value['fixed_id']\n",
    "                embedding_fixed_truncated = tf.nn.embedding_lookup(embedding, embedding_fixed_ids_truncated)\n",
    "                seq_true_length = embeddings_value['true_length']\n",
    "                attention['keys'] = embedding_fixed_truncated\n",
    "                attention['masks'] = tf.sequence_mask(seq_true_length, max_seq_length)\n",
    "                attention['query'] = embedding_fixed\n",
    "        print(attention)\n",
    "        # {'keys': <tf.Tensor 'embedding_lookup_1/Identity_1:0' shape=(None, 50, 128) dtype=float32>, 'masks': <tf.Tensor 'SequenceMask/Less:0' shape=(None, 50) dtype=bool>, 'query': <tf.Tensor 'Relu_1:0' shape=(None, 128) dtype=float32>}\n",
    "        # Dimensions must be equal, but are 5 and 128 for 'mul' (op: 'Mul') with input shapes: [4,5], [?,128].\n",
    "        attention_out = attention_module(attention['query'], attention['keys'], attention['masks'])\n",
    "        print(attention_out)\n",
    "        features = embeddings + [cont_features]\n",
    "        input_embedding = tf.concat(features, axis=1)\n",
    "\n",
    "        print(\"the shape of input_embedding is:\", input_embedding.shape)\n",
    "\n",
    "#         for i, num_layer in enumerate(num_layers):\n",
    "#             print(i,num_layer)\n",
    "#             input_embedding = tf.keras.layers.Dense(num_layer, activation=tf.nn.relu, name=f\"layer_{i}\")(input_embedding)\n",
    "#             if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#                 input_embedding = tf.nn.dropout(input_embedding, drop_out_rate, name=f\"layer_dropout_{i}\")\n",
    "                \n",
    "\n",
    "#             input_embedding = tf.layers.dense(input_embedding, num_layer, activation=tf.nn.relu,\n",
    "#                                   kernel_initializer=tf.initializers.random_normal(mean=0.0, stddev=0.1),\n",
    "#                                   bias_initializer=tf.initializers.random_normal(mean=0.0, stddev=0.1), name=f\"layer_{i}\")\n",
    "        #   input_embedding = deep_crossing(input_embedding)\n",
    "        input_embedding = wide_and_deep(input_embedding, wide_crossing, deep_crossing, combination_layer)\n",
    "        \n",
    "        logits = tf.keras.layers.Dense(1, name=\"layer_output\")(input_embedding)\n",
    "        probabilities = tf.nn.sigmoid(logits)  # num * 1\n",
    "        predictions = tf.cast(probabilities > 0.5, tf.float32)\n",
    "        \n",
    "#         logits = tf.matmul(user_vector, item_embedding, transpose_b=True)\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            predictions = {\n",
    "                \"probabilities\": probabilities,\n",
    "                \"predictions\": predictions\n",
    "            }\n",
    "            export_outputs = {\n",
    "                \"export_outputs\": tf.estimator.export.PredictOutput(predictions) # predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "        else:\n",
    "\n",
    "#             one_hot_labels = tf.one_hot(labels, num_items, dtype=tf.float32)\n",
    "            print(labels.shape, logits.shape)\n",
    "#             print(labels, logits)\n",
    "            cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits) # tf.expand_dims()\n",
    "            mean_loss = tf.reduce_mean(cross_entropy)\n",
    "#             print(cross_entropy, mean_loss)\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                optimizer = tf.compat.v1.train.AdamOptimizer()#GradientDescentOptimizer(learning_rate)\n",
    "                # Input to reshape is a tensor with 32 values, but the requested shape has 1\n",
    "    #             train_op = optimizer.minimize(cross_entropy, tf.compat.v1.train.get_or_create_global_step())\n",
    "                \n",
    "                train_op = optimizer.minimize(mean_loss, tf.compat.v1.train.get_or_create_global_step())\n",
    "                estimatorSpec = tf.estimator.EstimatorSpec(mode, loss=mean_loss, train_op=train_op)\n",
    "                return estimatorSpec\n",
    "\n",
    "            else: #mode == tf.estimator.ModeKeys.EVAL\n",
    "                def metric_fn(labels, predictions):\n",
    "                    return {\"accuracy\":tf.compat.v1.metrics.accuracy(labels, predictions)\n",
    "                           }\n",
    "                eval_metrics = metric_fn(labels, predictions)\n",
    "                estimatorSpec = tf.estimator.EstimatorSpec(mode=mode, loss=mean_loss, eval_metric_ops = eval_metrics)\n",
    "                return estimatorSpec\n",
    "\n",
    "    return model_fn\n",
    "\n",
    "def input_fn_builder(input_features, labels=None, batch_size=64, epoch_num=1, is_training=False, shuffle_buffer_size=None, seed=None):\n",
    "    # input_features = {'cont_features':[num_examples, num_cont_features], \n",
    "    #                   'embedding_features':{'space_id':{'fixed':[num_examples],\n",
    "    #                                                 'ragged':{'indices':[num_examples], 'dense_shape':[row, col]},\n",
    "#                                                     'truncated':{'fixed_ids':[num_examples], 'true_lengths':[num_examples]}\n",
    "    #                                                }}}\n",
    "    def input_fn():\n",
    "        features = {}\n",
    "#         print(input_features)\n",
    "        num_examples = input_features['num_examples']\n",
    "        if 'cont_features' in input_features:\n",
    "            features['cont_features'] = tf.constant(input_features['cont_features'], dtype=tf.float32)\n",
    "            \n",
    "        if 'embedding_features' in input_features:\n",
    "            features['embedding_features'] = {}\n",
    "            for space_id, embedding in input_features['embedding_features'].items():\n",
    "                fixed_ragged = {}\n",
    "                if 'univalent' in embedding:\n",
    "                    fixed_ragged['univalent'] = tf.constant(embedding['univalent'], dtype=tf.int32)\n",
    "                    \n",
    "                if 'multivalent' in embedding:\n",
    "                    ragged_indices = embedding['multivalent']['indices']\n",
    "                    indices = [[i, j] for i in range(num_examples) for j in range(len(ragged_indices[i]))]\n",
    "                    values = [x for row in ragged_indices for x in row]\n",
    "                    dense_shape = embedding['multivalent']['dense_shape'] # [num_examples, embedding['length']]\n",
    "                    sp_indexes = tf.SparseTensor(indices=indices, values=values, dense_shape=dense_shape)\n",
    "                    fixed_ragged['multivalent'] = sp_indexes\n",
    "                \n",
    "                features['embedding_features'][space_id] = fixed_ragged\n",
    "                \n",
    "        if labels is None:\n",
    "            data_set = tf.data.Dataset.from_tensor_slices(features)\n",
    "        else:\n",
    "            data_set = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "        if is_training:\n",
    "            print(f'epoch {epoch_num}')\n",
    "            data_set = data_set.repeat(epoch_num)\n",
    "            if shuffle_buffer_size is None:\n",
    "                data_set = data_set.shuffle(buffer_size=num_examples, seed=seed)\n",
    "            else:\n",
    "                data_set = data_set.shuffle(buffer_size=shuffle_buffer_size, seed=seed)\n",
    "        \n",
    "        data_set = data_set.prefetch(buffer_size=128)\n",
    "        data_set = data_set.batch(batch_size=batch_size)\n",
    "\n",
    "        return data_set\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_meta_data(num_example, num_cont_features, embedding_shapes, meta_file_path = 'train.meta'):\n",
    "    meta_map = {'embedding_space': embedding_shapes}\n",
    "    meta_map['num_example'] = num_example\n",
    "    meta_map['num_cont_features'] = num_cont_features\n",
    "    \n",
    "    with open(meta_file_path, \"w\") as file:\n",
    "        json.dump(meta_map, file)\n",
    "        \n",
    "def load_meta_data(meta_file_path = 'train.meta'):\n",
    "    with open(meta_file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def truncate_and_pad_seq(ids, max_length, paddings=0):\n",
    "    true_length = len(ids)\n",
    "    fixed_ids = ids[:max_length]\n",
    "    while len(fixed_ids) < max_length:\n",
    "        fixed_ids.append(paddings)\n",
    "        \n",
    "    return fixed_ids, true_length\n",
    "\n",
    "def truncate_and_pad_seqs(ids, max_length, paddings=0):\n",
    "    fixed_ids = []\n",
    "    true_lengths = []\n",
    "    for row in ids:\n",
    "        fixed_id, true_length = truncate_and_pad_seq(row, max_length, paddings)\n",
    "        fixed_ids.append(fixed_id)\n",
    "        true_lengths.append(true_length)\n",
    "    return fixed_ids, true_lengths\n",
    "    \n",
    "def df_to_input_features(df, embedding_space, cols_del, meta_file_path = 'train.meta', truncated_cols={'utvId_past_emb':50}):\n",
    "    input_features = {}\n",
    "#   embedding_space = [space_id: {'embedding_size':embedding_size, 'encoder':encoder,\n",
    "#                                   'univalent':col, 'multivalent':col}]\n",
    "#   truncated_cols = {'col':max_seq_size}\n",
    "\n",
    "    cont_cols = np.setdiff1d(df.columns, cols_del, assume_unique=True)\n",
    "    other_cols = []\n",
    "#     embedding_cols = {'space_id':{'fixed': col, 'ragged': col, 'length': n_class }}\n",
    "    embedding_features = {}\n",
    "    embedding_shapes = {}\n",
    "    for space_id, space in embedding_space.items():\n",
    "        embedding_features[space_id] = {}\n",
    "        num_classes = len(space['encoder'].classes_)\n",
    "        embedding_shapes[space_id] = {'embedding_shape':[num_classes, space['embedding_size']]}\n",
    "        if 'univalent' in space:\n",
    "            col = space['univalent']\n",
    "            embedding_features[space_id]['univalent'] = df[col].values\n",
    "            other_cols.append(col)\n",
    "            embedding_shapes[space_id]['univalent'] = col\n",
    "        if 'multivalent' in space:\n",
    "            col = space['multivalent']\n",
    "            indices = df[col].values\n",
    "            other_cols.append(col)\n",
    "            if truncated_cols is not None and col in truncated_cols:\n",
    "                max_seq_size = truncated_cols[col]\n",
    "                fixed_ids, true_lengths = truncate_and_pad_seqs(indices, max_seq_size)\n",
    "                embedding_features[space_id]['truncated'] = {'fixed_ids':fixed_ids, 'true_lengths':true_lengths}\n",
    "                embedding_shapes[space_id]['truncated'] = max_seq_size\n",
    "            else:\n",
    "                embedding_features[space_id]['multivalent'] = {'indices':indices, 'dense_shape':[df.shape[0], num_classes]}\n",
    "                embedding_shapes[space_id]['multivalent'] = col\n",
    "        \n",
    "    if len(embedding_features) > 0:\n",
    "        input_features['embedding_features'] = embedding_features\n",
    "    cont_cols = np.setdiff1d(cont_cols, other_cols, assume_unique=True)\n",
    "    print(cont_cols)\n",
    "    if len(cont_cols) > 0:\n",
    "        input_features['cont_features'] = df[cont_cols].values\n",
    "    input_features['num_examples'] = df.shape[0]\n",
    "    save_meta_data(input_features['num_examples'], df[cont_cols].values.shape[1], embedding_shapes, meta_file_path=meta_file_path)\n",
    "    return input_features, embedding_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is_weekend' 'duration_seconds_past_mean' 'duration_seconds_past_min'\n",
      " 'duration_seconds_past_max' 'likeNum' 'playNum' 'duration_seconds'\n",
      " 'now_uploadtime' 'example_age' 'example_age_square'\n",
      " 'example_age_rootsquare']\n"
     ]
    }
   ],
   "source": [
    "# df_train1 = pd.concat([df_train, df_test], axis=0)\n",
    "cols_del = ['uid','event_time_now','utvId_now','collect_day_now','utvId','y', 'uploadtime']\n",
    "input_features_train, embedding_shapes = df_to_input_features(df_train, embedding_space=embedding_space, cols_del=cols_del)\n",
    "y_train = df_train[['y']].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features_train['embedding_features']['utvid']['truncated']['true_lengths'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_features_train['embedding_features']['utvid']['truncated']['fixed_ids'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-22 11:01:01: Finished 0\n",
      "2020-09-22 11:01:06: Finished 50000\n",
      "2020-09-22 11:01:11: Finished 100000\n",
      "2020-09-22 11:01:15: Finished 150000\n",
      "2020-09-22 11:01:20: Finished 200000\n",
      "2020-09-22 11:01:24: Finished 250000\n",
      "2020-09-22 11:01:29: Finished 300000\n",
      "2020-09-22 11:01:33: Finished 350000\n",
      "2020-09-22 11:01:38: Finished 400000\n",
      "2020-09-22 11:01:42: Finished 450000\n",
      "2020-09-22 11:01:47: Finished 500000\n",
      "2020-09-22 11:01:51: Finished 550000\n",
      "2020-09-22 11:01:56: Finished 600000\n",
      "2020-09-22 11:02:00: Finished 650000\n",
      "2020-09-22 11:02:05: Finished 700000\n",
      "2020-09-22 11:02:09: Finished 750000\n",
      "2020-09-22 11:02:14: Finished 800000\n",
      "2020-09-22 11:02:18: Finished 850000\n",
      "2020-09-22 11:02:23: Finished 900000\n",
      "2020-09-22 11:02:27: Finished 950000\n",
      "2020-09-22 11:02:32: Finished 1000000\n",
      "2020-09-22 11:02:36: Finished 1050000\n",
      "2020-09-22 11:02:41: Finished 1100000\n",
      "2020-09-22 11:02:45: Finished 1150000\n",
      "2020-09-22 11:02:50: Finished 1200000\n",
      "2020-09-22 11:02:54: Finished 1250000\n",
      "2020-09-22 11:02:59: Finished 1300000\n",
      "2020-09-22 11:03:03: Finished 1350000\n",
      "2020-09-22 11:03:08: Finished 1400000\n",
      "2020-09-22 11:03:12: Finished 1450000\n",
      "2020-09-22 11:03:17: Finished 1500000\n",
      "2020-09-22 11:03:21: Finished 1550000\n",
      "2020-09-22 11:03:26: Finished 1600000\n",
      "2020-09-22 11:03:30: Finished 1650000\n",
      "2020-09-22 11:03:35: Finished 1700000\n",
      "2020-09-22 11:03:39: Finished 1750000\n",
      "2020-09-22 11:03:44: Finished 1800000\n",
      "2020-09-22 11:03:48: Finished 1850000\n",
      "2020-09-22 11:03:53: Finished 1900000\n",
      "2020-09-22 11:03:57: Finished 1950000\n",
      "2020-09-22 11:04:02: Finished 2000000\n",
      "2020-09-22 11:04:06: Finished 2050000\n",
      "2020-09-22 11:04:11: Finished 2100000\n",
      "2020-09-22 11:04:15: Finished 2150000\n",
      "2020-09-22 11:04:20: Finished 2200000\n",
      "2020-09-22 11:04:24: Finished 2250000\n",
      "2020-09-22 11:04:29: Finished 2300000\n",
      "2020-09-22 11:04:34: Finished 2350000\n",
      "2020-09-22 11:04:38: Finished 2400000\n",
      "2020-09-22 11:04:43: Finished 2450000\n",
      "2020-09-22 11:04:47: Finished 2500000\n",
      "2020-09-22 11:04:52: Finished 2550000\n",
      "2020-09-22 11:04:56: Finished 2600000\n",
      "2020-09-22 11:05:01: Finished 2650000\n",
      "2020-09-22 11:05:06: Finished 2700000\n",
      "2020-09-22 11:05:10: Finished 2750000\n"
     ]
    }
   ],
   "source": [
    "# features = {'cont_features': VarLenFeature, space_id+'_fixed': }\n",
    "def input_features_to_tfrecords(input_features, labels = None, filename = 'train.tfrecords'):\n",
    "    # input_features = {'cont_features':[num_examples, num_cont_features], \n",
    "    #                   'embedding_features':{'space_id':{'univalent':[num_examples], \n",
    "    #                                                 'multivalent':{'indices':[num_examples], 'dense_shape':[row, col]} \n",
    "    #                                                }},\n",
    "    #                   'num_examples': num_examples}\n",
    "    num_examples = input_features['num_examples']\n",
    "    cont_features = input_features['cont_features']\n",
    "    embedding_features = input_features['embedding_features']\n",
    "    \n",
    "    writer = tf.io.TFRecordWriter(filename)\n",
    "    for i in range(num_examples):\n",
    "        if i % 50000 == 0:\n",
    "            utils.print_with_datetime(f'Finished {i}')\n",
    "        \n",
    "        cont_feat = tf.train.Feature(float_list=tf.train.FloatList(value=cont_features[i]))\n",
    "        feat_map = {'cont_features': cont_feat}\n",
    "        for space_id, space in embedding_features.items():\n",
    "            feat_list = []\n",
    "            if 'univalent' in space:\n",
    "                feat_map[space_id+'_univalent'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[space['univalent'][i]]))\n",
    "            if 'multivalent' in space:\n",
    "                feat_map[space_id+'_multivalent'] = tf.train.Feature(int64_list=tf.train.Int64List(value=space['multivalent']['indices'][i]))\n",
    "            if 'truncated' in space:\n",
    "                feat_map[space_id+'_fixed_id'] = tf.train.Feature(int64_list=tf.train.Int64List(value=space['truncated']['fixed_ids'][i]))\n",
    "                feat_map[space_id+'_true_length'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[space['truncated']['true_lengths'][i]]))\n",
    "        if labels is not None:\n",
    "            feat_map['labels'] = tf.train.Feature(float_list = tf.train.FloatList(value=labels[i]))\n",
    "        \n",
    "        features = tf.train.Features(feature=feat_map)\n",
    "        tf_example = tf.train.Example(features=features)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "input_features_to_tfrecords(input_features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decode_map(embedding_space, is_training=False, len_cont_feats=11):\n",
    "    decode_map = {'cont_features': tf.io.FixedLenFeature([len_cont_feats], tf.float32)}\n",
    "    for space_id, space in embedding_space.items():\n",
    "        if 'univalent' in space:\n",
    "            decode_map[space_id+'_univalent'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "        if 'multivalent' in space:\n",
    "            decode_map[space_id+'_multivalent'] = tf.io.VarLenFeature(tf.int64)\n",
    "        if 'truncated' in space:\n",
    "            max_seq_size = space['truncated']\n",
    "            decode_map[space_id+'_fixed_id'] = tf.io.FixedLenFeature([max_seq_size], tf.int64)\n",
    "            decode_map[space_id+'_true_length'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "            \n",
    "    if is_training:\n",
    "        decode_map['labels'] = tf.io.FixedLenFeature([1], tf.float32)\n",
    "    return decode_map\n",
    "\n",
    "\n",
    "def example_to_model_input(example, embedding_space):\n",
    "#   features = {'cont_features':tensor, \n",
    "#               'embedding_features':{space_id: {'univalent':tensor, 'multivalent':sparse_tensor}}}\n",
    "#     embedding_space=meta['embedding_space']\n",
    "#     attention=meta['attention']\n",
    "    # {attention:{query:{key:value}}}, e.g. attention:{'utvid_univalent':{'utvid_multivalent':'utvid_multivalent'}}}\n",
    "    # attention={'query':'utvid_univalent', 'key':'utvid_multivalent'}\n",
    "    features = {}\n",
    "    if 'cont_features' in example:\n",
    "        features['cont_features'] = example['cont_features']\n",
    "    if embedding_space:\n",
    "        features['embedding_features'] = {}\n",
    "    for space_id, values in embedding_space.items():\n",
    "        features['embedding_features'][space_id] = {}\n",
    "#         print(space_id, values)\n",
    "        if 'univalent' in values:\n",
    "            features['embedding_features'][space_id]['univalent'] = example[space_id+'_univalent']\n",
    "        if 'multivalent' in values:\n",
    "            features['embedding_features'][space_id]['multivalent'] = example[space_id+'_multivalent']\n",
    "        if 'truncated' in values:\n",
    "            features['embedding_features'][space_id]['fixed_id'] = example[space_id+'_fixed_id']\n",
    "            features['embedding_features'][space_id]['true_length'] = example[space_id+'_true_length']\n",
    "    labels = None\n",
    "    if 'labels' in example:\n",
    "        labels = example['labels']\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def parse_nested_dict(example_proto, meta, is_training=False):\n",
    "    decode_map = get_decode_map(meta['embedding_space'], is_training, meta['num_cont_features'])\n",
    "    example = tf.io.parse_single_example(example_proto, features=decode_map)\n",
    "\n",
    "    model_input_features, labels = example_to_model_input(example, meta['embedding_space'])\n",
    "    if labels is not None:\n",
    "        return model_input_features, labels\n",
    "    else:\n",
    "        return model_input_features\n",
    "\n",
    "def input_fn_builder_tf(meta, tfrecord_path='train.tfrecords',  batch_size=64, epoch_num=1, is_training=False, \n",
    "                        shuffle_buffer_size=None, seed=None):\n",
    "    # input_features = {'cont_features':[num_examples, num_cont_features], \n",
    "    #                   'embedding_features':{'space_id':{'fixed':[num_examples],\n",
    "    #                                                 'ragged':{'indices':[num_examples], 'dense_shape':[row, col]}\n",
    "    #                                                }}}\n",
    "    def input_fn():\n",
    "        data_set = tf.data.TFRecordDataset(tfrecord_path)\n",
    "        data_set = data_set.map(lambda x: parse_nested_dict(x, meta, is_training))\n",
    "        if is_training:\n",
    "            print(f'epoch {epoch_num}')\n",
    "            data_set = data_set.repeat(epoch_num)\n",
    "            if shuffle_buffer_size is None:\n",
    "                data_set = data_set.shuffle(buffer_size=train_meta['num_example'], seed=seed)\n",
    "            else:\n",
    "                data_set = data_set.shuffle(buffer_size=shuffle_buffer_size, seed=seed)\n",
    "        \n",
    "        data_set = data_set.batch(batch_size=batch_size).prefetch(buffer_size=1)\n",
    "\n",
    "        return data_set\n",
    "    return input_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_space': {'tagId': {'embedding_shape': [152, 4],\n",
       "   'multivalent': 'tagId_emb'},\n",
       "  'utvid': {'embedding_shape': [445, 128],\n",
       "   'univalent': 'utvId_now_emb',\n",
       "   'truncated': 50},\n",
       "  'authorId': {'embedding_shape': [42, 4], 'univalent': 'authorId_emb'},\n",
       "  'areaId': {'embedding_shape': [13, 3], 'univalent': 'areaId_emb'}},\n",
       " 'num_example': 2769495,\n",
       " 'num_cont_features': 11}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_score(query, key):\n",
    "    # score(query, key) = W_2*PReLU(W_1*concate([query, key, query-key, query*key]))\n",
    "    score = tf.concat([query, key, key-query, key*query], axis=-1) \n",
    "    score = tf.keras.layers.Dense(36, activation=tf.nn.relu)(score)\n",
    "    score = tf.keras.layers.Dense(1)(score)\n",
    "    return score\n",
    "\n",
    "def attention_module(query, keys, masks, values=None):\n",
    "    '''\n",
    "    queries:    [B, H]\n",
    "    keys:       [B, T, H]\n",
    "    masks:      [B, T]\n",
    "    '''\n",
    "    if values is None:\n",
    "        values = keys\n",
    "        \n",
    "    emb_size = query.shape[-1]\n",
    "#     weights = att_score(query, keys) # [B, T, 1]\n",
    "    weights = key*query # [B, T, H]\n",
    "    weights = tf.reduce_sum(weights, -1) # [B, T, 1]\n",
    "    neg_inf = -2 ** 32 + 1\n",
    "    weights = tf.where(masks, weights, neg_inf)\n",
    "    # scaled dot product\n",
    "    weights = weights / (emb_size ** 0.5) # 我觉得可以在att_score的query*key中scale, 其它不用scale\n",
    "    \n",
    "    return values * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'rank_dcn/', '_tf_random_seed': 2019, '_save_summary_steps': 100, '_save_checkpoints_steps': 30000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2564198208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "epoch 1\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "{'keys': <tf.Tensor 'embedding_lookup_1/Identity_1:0' shape=(None, 50, 128) dtype=float32>, 'masks': <tf.Tensor 'SequenceMask/Less:0' shape=(None, 50) dtype=bool>, 'query': <tf.Tensor 'Relu_1:0' shape=(None, 128) dtype=float32>}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 5 and 128 for 'mul' (op: 'Mul') with input shapes: [4,5], [?,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 5 and 128 for 'mul' (op: 'Mul') with input shapes: [4,5], [?,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0674b90d013f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, steps=100 , max_steps=10200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m# eval_result = estimator.evaluate(input_fn=eval_input_fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# eval_result = estimator.train_and_evaluate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1186\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1188\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1189\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-3c3272ff04a4>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mattention_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keys'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'masks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcont_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-fa6c00aa6f77>\u001b[0m in \u001b[0;36mattention_module\u001b[0;34m(query, keys, masks, values)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0memb_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     weights = att_score(query, keys) # [B, T, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;31m# [B, T, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B, T, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mneg_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[0;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1178\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6488\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6489\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6490\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6491\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6492\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3294\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3295\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3296\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3297\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1712\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1713\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1714\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 5 and 128 for 'mul' (op: 'Mul') with input shapes: [4,5], [?,128]."
     ]
    }
   ],
   "source": [
    "epoch_num = 1 #25\n",
    "batch_size = 64\n",
    "seed = 2019\n",
    "num_layers =  [512, 256, 128] #[512,256,128] # [256,128] # 0.7153 0.747 \n",
    "train_meta = load_meta_data()\n",
    "embedding_shapes = train_meta['embedding_space']\n",
    "model_fn = model_fn_builder(embedding_shapes=embedding_shapes, num_layers=num_layers)\n",
    "\n",
    "# train_input_fn = input_fn_builder(input_features_train, y_train, batch_size, epoch_num, is_training=True, seed=seed)\n",
    "train_input_fn = input_fn_builder_tf(train_meta, batch_size=batch_size, epoch_num=epoch_num, is_training=True, seed=seed)\n",
    "eval_input_fn = train_input_fn\n",
    "# train_input_fn = input_fn_builder(X_train, cont_features_train, uid_features_train,y_train, \n",
    "#                                   len(encoder.classes_),batch_size, epoch_num, is_training=True, seed=seed)\n",
    "# eval_input_fn = input_fn_builder(X_eval, cont_features_eval, uid_features_test, y_eval, len(encoder.classes_), batch_size) \n",
    "# eval_input_fn = input_fn_builder(X_eval_clean, cont_features_eval_clean, uid_features_test_clean, y_eval_clean, num_items, batch_size) \n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "    model_dir=\"rank_dcn/\",\n",
    "    tf_random_seed=seed,\n",
    "    save_checkpoints_steps=30000,\n",
    "    keep_checkpoint_max=5,\n",
    "    log_step_count_steps=5000\n",
    ")\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\n",
    "\n",
    "# train_spec = tf.estimator.TrainSpec(\n",
    "#     input_fn=train_input_fn,\n",
    "#     max_steps=1000\n",
    "# )\n",
    "\n",
    "train_result = estimator.train(input_fn=train_input_fn) #, steps=100 , max_steps=10200\n",
    "# eval_result = estimator.evaluate(input_fn=eval_input_fn)\n",
    "# eval_result = estimator.train_and_evaluate()\n",
    "utils.print_with_datetime(\"***** Eval results *****\")\n",
    "# for key in sorted(eval_result.keys()):\n",
    "#     print(key + '='+ str(eval_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k_utvid (43644, 30)\n",
      "arr_topk (1309320, 11)\n",
      "43644 30\n",
      "2020-09-22 11:43:40: Finished getting 0 top 30 data\n",
      "2020-09-22 11:43:52: Finished getting 5000 top 30 data\n",
      "2020-09-22 11:44:04: Finished getting 10000 top 30 data\n",
      "2020-09-22 11:44:16: Finished getting 15000 top 30 data\n",
      "2020-09-22 11:44:28: Finished getting 20000 top 30 data\n",
      "2020-09-22 11:44:39: Finished getting 25000 top 30 data\n",
      "2020-09-22 11:44:51: Finished getting 30000 top 30 data\n",
      "2020-09-22 11:45:03: Finished getting 35000 top 30 data\n",
      "2020-09-22 11:45:15: Finished getting 40000 top 30 data\n",
      "user_time_item (1309320, 10)\n",
      "2020-09-22 11:45:25: Processing user_time_item\n",
      "(1309320, 4)\n",
      "{'uid': 'uid', 'event_time_now': 'event_time_now'} Index(['utvId_now_emb', 'uid', 'event_time_now', 'utvId_now'], dtype='object')\n",
      "user_time (599132, 9) Index(['areaId_emb', 'is_weekend', 'utvId_past_emb',\n",
      "       'duration_seconds_past_mean', 'duration_seconds_past_min',\n",
      "       'duration_seconds_past_max', 'event_time_now', 'uid',\n",
      "       'collect_day_now'],\n",
      "      dtype='object')\n",
      "(1309320, 11)\n",
      "{'utvId_now': 'utvId'} Index(['utvId_now_emb', 'uid', 'event_time_now', 'utvId_now', 'areaId_emb',\n",
      "       'is_weekend', 'utvId_past_emb', 'duration_seconds_past_mean',\n",
      "       'duration_seconds_past_min', 'duration_seconds_past_max',\n",
      "       'collect_day_now'],\n",
      "      dtype='object')\n",
      "item (939, 7) Index(['authorId_emb', 'uploadtime', 'tagId_emb', 'likeNum', 'playNum',\n",
      "       'duration_seconds', 'utvId'],\n",
      "      dtype='object')\n",
      "(1309320, 18)\n"
     ]
    }
   ],
   "source": [
    "def get_recall_topk_user_time_item(df, encoder, k = 30):\n",
    "    recall_pred_probs = pd.read_csv('df_pred_probs2.csv',header=None).values\n",
    "    top_k_utvid = tf.nn.top_k(recall_pred_probs, k=k).indices.numpy()\n",
    "    print('top_k_utvid',top_k_utvid.shape) # top_k_utvid (43644, 30)\n",
    "    top_k_utvid = top_k_utvid.flatten()\n",
    "    arr_topk = np.empty_like(df, shape=(top_k_utvid.shape[0], df.shape[1]))\n",
    "    print('arr_topk', arr_topk.shape) # arr_topk (1309320, 10)\n",
    "    print(len(df), k) # 1198264 30\n",
    "    for i in range(len(df)):\n",
    "        if i % 5000 == 0:\n",
    "            utils.print_with_datetime(f'Finished getting {i} top {k} data')\n",
    "        offset = i * k\n",
    "        for j in range(k):\n",
    "            o = offset + j\n",
    "            for c in range(len(df.columns)):\n",
    "                arr_topk[o,c] = df.iloc[i,c]\n",
    "    df_topk = pd.DataFrame(arr_topk, columns=df.columns)\n",
    "    df_topk['utvId_now'] = encoder.back_transform(top_k_utvid)\n",
    "    return df_topk\n",
    "\n",
    "def get_topk_data(df):\n",
    "    user_time_item = {}\n",
    "    user_time_item['categorical'] = {'univalent':{'embedding': [{'space_id':'utvid', 'col':'utvId_now'}]}}\n",
    "    user_time_item['reference'] = {'user_time':{'uid':'uid', 'event_time_now':'event_time_now'},\n",
    "                                   'item':{'utvId_now':'utvId'}}\n",
    "    nested_data = [{'data':df, 'features': {'user_time_item':user_time_item}}]\n",
    "\n",
    "    return nested_data\n",
    "\n",
    "utvid_encoder = embedding_space['utvid']['encoder']\n",
    "k=30\n",
    "df_train_clean, df_test_clean = split_fn(df_user_time_item) # [df_user_time_item['y']==1]\n",
    "df_test_clean = df_test_clean[df_test_clean['utvId_now'].isin(df_train_clean['utvId_now'])]\n",
    "df_topk = get_recall_topk_user_time_item(df_test_clean, utvid_encoder, k=k)\n",
    "del df_topk['y']\n",
    "topk_data = get_topk_data(df_topk)\n",
    "df_pred, _, _, _, _, _ = process_data(topk_data, need_scale=True, cont_cols=cont_cols, scaler=scaler,embedding_space=embedding_space, pk=pk)\n",
    "col_fn_map = {'now_uploadtime': lambda x: x['event_time_now']//1e3 - datetime.datetime.strptime(str(x['uploadtime']), '%Y-%m-%d %H:%M:%S').timestamp(),\n",
    "              'example_age': lambda x: 0, \n",
    "              'example_age_square': lambda x: 0,\n",
    "              'example_age_rootsquare': lambda x: 0}\n",
    "df_pred, _ = postprocess(df_pred, col_fn_map, scaler=post_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is_weekend' 'duration_seconds_past_mean' 'duration_seconds_past_min'\n",
      " 'duration_seconds_past_max' 'likeNum' 'playNum' 'duration_seconds'\n",
      " 'now_uploadtime' 'example_age' 'example_age_square'\n",
      " 'example_age_rootsquare']\n",
      "2020-09-22 11:47:14: Finished 0\n",
      "2020-09-22 11:47:18: Finished 50000\n",
      "2020-09-22 11:47:22: Finished 100000\n",
      "2020-09-22 11:47:26: Finished 150000\n",
      "2020-09-22 11:47:30: Finished 200000\n",
      "2020-09-22 11:47:34: Finished 250000\n",
      "2020-09-22 11:47:38: Finished 300000\n",
      "2020-09-22 11:47:43: Finished 350000\n",
      "2020-09-22 11:47:47: Finished 400000\n",
      "2020-09-22 11:47:51: Finished 450000\n",
      "2020-09-22 11:47:55: Finished 500000\n",
      "2020-09-22 11:47:59: Finished 550000\n",
      "2020-09-22 11:48:04: Finished 600000\n",
      "2020-09-22 11:48:08: Finished 650000\n",
      "2020-09-22 11:48:12: Finished 700000\n",
      "2020-09-22 11:48:16: Finished 750000\n",
      "2020-09-22 11:48:20: Finished 800000\n",
      "2020-09-22 11:48:24: Finished 850000\n",
      "2020-09-22 11:48:28: Finished 900000\n",
      "2020-09-22 11:48:33: Finished 950000\n",
      "2020-09-22 11:48:37: Finished 1000000\n",
      "2020-09-22 11:48:41: Finished 1050000\n",
      "2020-09-22 11:48:45: Finished 1100000\n",
      "2020-09-22 11:48:49: Finished 1150000\n",
      "2020-09-22 11:48:53: Finished 1200000\n",
      "2020-09-22 11:48:58: Finished 1250000\n",
      "2020-09-22 11:49:02: Finished 1300000\n"
     ]
    }
   ],
   "source": [
    "test_tfrecord_path = 'test.tfrecords'\n",
    "test_meta_path = 'test.meta'\n",
    "input_features_pred, _ = df_to_input_features(df_pred, embedding_space=embedding_space, cols_del=cols_del, meta_file_path=test_meta_path)\n",
    "input_features_to_tfrecords(input_features_pred, filename=test_tfrecord_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "{}\n",
      "the shape of input_embedding is: (None, 150)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from rank_dcn/model.ckpt-43274\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "pred_meta = load_meta_data(test_meta_path)\n",
    "# pred_input_fn = input_fn_builder_tf(pred_meta, tfrecord_path=test_tfrecord_path, batch_size=512)\n",
    "pred_input_fn = input_fn_builder(input_features_pred, batch_size=512)\n",
    "\n",
    "pred = estimator.predict(input_fn=pred_input_fn)\n",
    "pred_probs = []\n",
    "for output in pred:\n",
    "#     print(output)\n",
    "    pred_probs.append(output['probabilities']) # np.contecate\n",
    "# pred_probs = pred_probs.reshape((X_topk.shape[0],1))\n",
    "# pred_probs = pred_probs.reshape((pred_probs.shape[0]//k, k))\n",
    "# pred_probs = pred_probs.reshape(pred_probs.shape[0] * k)\n",
    "pred_probs = np.array(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43644, 445)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utvid_encoder = embedding_space['utvid']['encoder']\n",
    "target_ids_eval_clean = np.array(utvid_encoder.transform(df_test_clean['utvId_now']))\n",
    "# target_ids_topk = df_pred['utvId_now_emb'].values\n",
    "target_ids_topk =  np.array(utvid_encoder.transform(df_pred['utvId_now'].values))\n",
    "n = pred_probs.shape[0]//k\n",
    "sparse_indices = [[i, target_ids_topk[i*k + j]] for i in range(n) for j in range(k) ]\n",
    "num_items = embedding_shapes['utvid']['embedding_shape'][0] \n",
    "pred_probs_n_hot = sparseTensor(sparse_indices, pred_probs, (n, num_items), dtype=np.float16)\n",
    "pred_probs_n_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-22 14:18:01: top 1 accuracy is 0.07382458069837779\n",
      "2020-09-22 14:18:01: top 5 accuracy is 0.32969022087801303\n",
      "2020-09-22 14:18:01: top 10 accuracy is 0.5417697736229493\n",
      "2020-09-22 14:18:01: top 20 accuracy is 0.7596920538905692\n",
      "2020-09-22 14:18:01: top 30 accuracy is 0.8318669232884245\n"
     ]
    }
   ],
   "source": [
    "for k in [1,5,10,20,30]: #,50,100\n",
    "    top_k_acc = tf.metrics.top_k_categorical_accuracy(tf.one_hot(target_ids_eval_clean, num_items), pred_probs_n_hot, k=k).numpy()\n",
    "    utils.print_with_datetime(f'top {k} accuracy is {top_k_acc.sum() / top_k_acc.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-18 14:24:59: top 1 accuracy is 0.0947209238383283\n",
      "2020-09-18 14:25:00: top 5 accuracy is 0.4252130877096508\n",
      "2020-09-18 14:25:01: top 10 accuracy is 0.6518650902758684\n",
      "2020-09-18 14:25:01: top 20 accuracy is 0.7994913390156723\n",
      "2020-09-18 14:25:02: top 30 accuracy is 0.8341352763266429\n"
     ]
    }
   ],
   "source": [
    "for topk in [1,5,10,20,30]:\n",
    "    max_k_preds = pred_probs_n_hot.argsort(axis=1)[:, -topk:][:, ::-1] \n",
    "    match_array = np.logical_or.reduce(max_k_preds==target_ids_eval_clean.reshape((target_ids_eval_clean.shape[0],1)), axis=1) \n",
    "    utils.print_with_datetime(f'top {topk} accuracy is {match_array.sum() / match_array.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-17 14:27:25: top 1 accuracy is 0.14001924663183943\n",
      "2020-09-17 14:27:26: top 5 accuracy is 0.35221336266153425\n",
      "2020-09-17 14:27:26: top 10 accuracy is 0.40695170011914583\n",
      "2020-09-17 14:27:27: top 20 accuracy is 0.43440106314728255\n",
      "2020-09-17 14:27:28: top 30 accuracy is 0.4411373842910824\n"
     ]
    }
   ],
   "source": [
    "for k in [1,5,10,20,30]: #,50,100\n",
    "    max_k_preds = pred_probs_n_hot.argsort(axis=1)[:, -k:][:, ::-1] \n",
    "    match_array = np.logical_or.reduce(max_k_preds==target_ids_eval_clean.reshape((target_ids_eval_clean.shape[0],1)), axis=1) \n",
    "    utils.print_with_datetime(f'top {k} accuracy is {match_array.sum() / match_array.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1 accuracy is 0.2572174869397855\n",
      "top 5 accuracy is 0.5768948767299056\n",
      "top 10 accuracy is 0.701402254605444\n",
      "top 20 accuracy is 0.7909907432865915\n",
      "top 30 accuracy is 0.8318669232884245\n",
      "top 50 accuracy is 0.8720557235817066\n",
      "top 100 accuracy is 0.9161167628998259\n"
     ]
    }
   ],
   "source": [
    "recall_pred_probs = pd.read_csv('df_pred_probs2.csv',header=None).values\n",
    "for k in [1,5,10,20,30,50,100]:\n",
    "    max_k_preds = recall_pred_probs.argsort(axis=1)[:, -k:][:, ::-1] \n",
    "    match_array = np.logical_or.reduce(max_k_preds==target_ids_eval_clean.reshape((target_ids_eval_clean.shape[0],1)), axis=1) \n",
    "    print(f'top {k} accuracy is {match_array.sum() / match_array.shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
